import os
import json
import subprocess
from pathlib import Path
from collections import defaultdict
import glob
import base64
import requests
from PIL import Image
from tqdm import tqdm
import pandas as pd
import time
import re

# ---- LLM: Jina DeepSearch (OpenAI-compatible) ----
from openai import OpenAI

import cv2
import torch
import torch.nn as nn
import torchvision.transforms as T
from torchvision.models.video import r2plus1d_18
from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights
from timm import create_model
from ultralytics import YOLO
import numpy as np


# =============================
# CONFIG
# =============================

BASE_DIR = Path(".").resolve()

# Main video for event detection + commentary
VIDEO_PATH = BASE_DIR / "endvideo.mp4"

FRAMES_DIR = BASE_DIR / "frames"
CLIPS_DIR = BASE_DIR / "clips"

# Model weights
YOLO_WEIGHTS   = BASE_DIR / "models" / "yolo-best.pt"
SHOT_WEIGHTS   = BASE_DIR / "models" / "best_shot_3class.pth"
UMPIRE_WEIGHTS = BASE_DIR / "models" / "best_umpire.pth"        # your umpire checkpoint
RUNOUT_WEIGHTS = BASE_DIR / "models" / "best_runout_3class.pth"
R2P1D_WEIGHTS  = BASE_DIR / "models" / "R(2+1)best.pt"

# Optional JSON metadata files (if you have them)
SHOT_META_JSON   = BASE_DIR / "models" / "best_shot_3class.json"
UMPIRE_META_JSON = BASE_DIR / "models" / "best_umpire.json"
RUNOUT_META_JSON = BASE_DIR / "models" / "best_runout_3class.json"
R2P1D_META_JSON  = BASE_DIR / "models" / "R(2+1)best.json"

# Output files
RAW_RESULTS_JSON = BASE_DIR / "model_outputs.json"
TIMELINE_JSON    = BASE_DIR / "timeline_for_llm.json"
PROMPT_TXT       = BASE_DIR / "commentary_prompt.txt"

# ðŸ”¹ Scorecard OCR outputs
SCORE_JSON       = BASE_DIR / "score_data.json"
SCORE_CSV        = BASE_DIR / "score_data.csv"

# ðŸ”¹ TTS output
TTS_OUTPUT       = BASE_DIR / "commentary_audio.mp3"

# ffmpeg settings
FRAME_RATE  = 1   # 1 frame per second from ffmpeg
CLIP_LENGTH = 6   # 6-second clips

# ðŸ”¹ New: subsample heavy processing
# e.g. 700 extracted frames -> 700 / 2 = 350 frames sent to OCR + models
FRAME_SUBSAMPLE = 2  # use every 2nd frame (set to 1 to use all)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ===== LLM (Jina DeepSearch â€“ OpenAI-compatible Chat API) =====
JINA_MODEL_ID = "jina-deepsearch-v1"
JINA_BASE_URL = "https://deepsearch.jina.ai/v1"

# ===== OCR.Space API Key =====
OCRSPACE_API_KEY = os.getenv("OCRSPACE_API_KEY", "YOUR_OCRSPACE_API_KEY")

# ===== ElevenLabs TTS =====
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
ELEVENLABS_VOICE_ID = os.getenv("ELEVENLABS_VOICE_ID")  # must be set in env
ELEVENLABS_MODEL_ID = "eleven_multilingual_v2"

# Crop region (bottom portion below the white line)
ROI_FRACTION_TOP = 0.80    # crop starts from 80% of image height
ROI_FRACTION_BOTTOM = 1.0  # bottom edge

# OCR.Space endpoint
VISION_URL = "https://api.ocr.space/parse/image"

# Retry settings
MAX_RETRIES = 5
INITIAL_BACKOFF = 1.0  # seconds

# Filled at runtime from metadata / checkpoints
SHOT_CLASSES   = []
UMPIRE_CLASSES = []
RUNOUT_CLASSES = []
VIDEO_CLASSES  = []


# =============================
# STEP 1: SPLIT VIDEO USING FFMPEG
# =============================

def run_ffmpeg_split(video_path: Path, frames_dir: Path, clips_dir: Path,
                     frame_rate: int = 1, clip_len: int = 6):
    frames_dir.mkdir(parents=True, exist_ok=True)
    clips_dir.mkdir(parents=True, exist_ok=True)

    # Frames @ frame_rate FPS
    frame_pattern = str(frames_dir / "frame_%06d.jpg")
    cmd_frames = [
        "ffmpeg", "-y",
        "-i", str(video_path),
        "-vf", f"fps={frame_rate}",
        frame_pattern,
    ]
    print("Running ffmpeg for frames:", " ".join(cmd_frames))
    subprocess.run(cmd_frames, check=True)

    # 6-sec clips
    clip_pattern = str(clips_dir / "clip_%06d.mp4")
    cmd_clips = [
        "ffmpeg", "-y",
        "-i", str(video_path),
        "-c", "copy",
        "-map", "0",
        "-segment_time", str(clip_len),
        "-f", "segment",
        clip_pattern,
    ]
    print("Running ffmpeg for clips:", " ".join(cmd_clips))
    subprocess.run(cmd_clips, check=True)


# =============================
# FRAME SAMPLING HELPERS (USED BY MODELS + OCR)
# =============================

def get_sampled_frame_paths(frames_dir: Path, step: int = 1):
    """
    Returns a subsampled, sorted list of frame Paths.
    If step=2 â†’ every 2nd frame; if step=1 â†’ all frames.
    """
    all_frames = sorted(frames_dir.glob("frame_*.jpg"))
    if step <= 1:
        return all_frames
    return all_frames[::step]


def frame_index_from_name(path: Path) -> int:
    """
    Parse original 0-based frame index from filename frame_000001.jpg.
    ffmpeg creates 1-based numbering, so index = number - 1.
    """
    m = re.search(r"frame_(\d+)\.jpg", path.name)
    if not m:
        # Fallback: 0 if pattern not matched
        return 0
    return int(m.group(1)) - 1


# =============================
# STEP 2: MODEL LOADING
# =============================

def load_yolo_model(weights_path: Path):
    print(f"Loading YOLOv8 from {weights_path}")
    return YOLO(str(weights_path))


def _load_json_meta_if_exists(meta_path: Path):
    if meta_path is not None and meta_path.exists():
        with open(meta_path, "r") as f:
            return json.load(f)
    return None


def load_efficientnet_classifier(weights_path: Path,
                                 meta_json_path: Path | None = None):
    """
    Generic EfficientNet-based image classifier loader (for SHOT and RUNOUT).
    """
    print(f"Loading EfficientNet classifier from {weights_path}")

    model_name = "efficientnet_b0"
    class_names = None
    meta = _load_json_meta_if_exists(meta_json_path)
    if meta is not None:
        class_names = meta.get("class_names")
        model_name = meta.get("model_name", model_name)

    ckpt = torch.load(str(weights_path), map_location=DEVICE)

    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        state_dict = ckpt["state_dict"]
        if class_names is None and "class_names" in ckpt:
            class_names = ckpt["class_names"]
        if "model_name" in ckpt:
            model_name = ckpt["model_name"]
    elif isinstance(ckpt, dict) and "model_state_dict" in ckpt:
        state_dict = ckpt["model_state_dict"]
        if class_names is None and "class_names" in ckpt:
            class_names = ckpt["class_names"]
        if "model_name" in ckpt:
            model_name = ckpt["model_name"]
    else:
        state_dict = ckpt

    if class_names is None:
        raise RuntimeError(
            f"Could not find class_names for {weights_path}. "
            f"Please provide a JSON with 'class_names', or include it in the checkpoint."
        )

    num_classes = len(class_names)

    print(f"  -> model_name={model_name}, num_classes={num_classes}")
    print(f"  -> class_names={class_names}")

    base_model_name = model_name
    if model_name.startswith("efficientnet_b0") and model_name != "efficientnet_b0":
        print(f"  -> Detected custom variant '{model_name}', using backbone 'efficientnet_b0'")
        base_model_name = "efficientnet_b0"

    model = create_model(base_model_name, pretrained=False, num_classes=num_classes)
    model.load_state_dict(state_dict)
    model.to(DEVICE)
    model.eval()
    return model, class_names


# =============================
# Umpire model architecture
# =============================

class UmpireEfficientNetClassifier(nn.Module):
    def __init__(self, num_classes: int):
        super().__init__()
        weights = EfficientNet_B0_Weights.DEFAULT
        self.base = efficientnet_b0(weights=weights)
        in_feat = self.base.classifier[1].in_features
        self.base.classifier = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(in_feat, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, num_classes),
        )

    def forward(self, x):
        return self.base(x)


def load_umpire_model(weights_path: Path,
                      meta_json_path: Path | None = None):
    print(f"Loading Umpire EfficientNet model from {weights_path}")

    class_names = None
    meta = _load_json_meta_if_exists(meta_json_path)
    if meta is not None:
        class_names = meta.get("class_names")

    ckpt = torch.load(str(weights_path), map_location=DEVICE)

    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        state_dict = ckpt["state_dict"]
        if class_names is None and "class_names" in ckpt:
            class_names = ckpt["class_names"]
    else:
        state_dict = ckpt

    if class_names is None:
        raise RuntimeError(
            f"[UMPIRE] Could not find class_names for {weights_path}. "
            f"Ensure the checkpoint or meta JSON has 'class_names'."
        )

    num_classes = len(class_names)
    print(f"  -> num_classes={num_classes}")
    print(f"  -> class_names={class_names}")

    model = UmpireEfficientNetClassifier(num_classes).to(DEVICE)
    model.load_state_dict(state_dict)
    model.eval()
    return model, class_names


def load_r2plus1d_model(weights_path: Path,
                        meta_json_path: Path | None = None):
    print(f"Loading R(2+1)D model from {weights_path}")

    class_names = None
    meta = _load_json_meta_if_exists(meta_json_path)
    if meta is not None:
        class_names = meta.get("class_names")

    ckpt = torch.load(str(weights_path), map_location=DEVICE)

    if isinstance(ckpt, dict) and "state_dict" in ckpt:
        state_dict = ckpt["state_dict"]
        if class_names is None and "class_names" in ckpt:
            class_names = ckpt["class_names"]
    elif isinstance(ckpt, dict) and "model_state_dict" in ckpt:
        state_dict = ckpt["model_state_dict"]
        if class_names is None and "class_names" in ckpt:
            class_names = ckpt["class_names"]
    else:
        state_dict = ckpt

    if class_names is None:
        raise RuntimeError(
            f"Could not find class_names for {weights_path}. "
            f"Please provide a JSON with 'class_names', or include it in the checkpoint."
        )

    num_classes = len(class_names)
    print(f"  -> num_classes={num_classes}")
    print(f"  -> class_names={class_names}")

    model = r2plus1d_18(weights=None)
    in_feats = model.fc.in_features
    model.fc = nn.Linear(in_feats, num_classes)

    model.load_state_dict(state_dict)
    model.to(DEVICE)
    model.eval()
    return model, class_names


# =============================
# STEP 3: PREPROCESSING
# =============================

image_transform = T.Compose([
    T.ToPILImage(),
    T.Resize((224, 224)),
    T.ToTensor(),
])


def load_video_as_tensor(video_path: Path,
                         num_frames: int = 16,
                         resize_hw=(112, 112)) -> torch.Tensor:
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    if total_frames <= 0:
        cap.release()
        raise RuntimeError(f"No frames in video: {video_path}")

    indices = torch.linspace(0, total_frames - 1, steps=num_frames).long().tolist()

    frames = []
    cur_idx = 0
    target_ptr = 0
    target_idx = indices[target_ptr]

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if cur_idx == target_idx:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame_resized = cv2.resize(frame_rgb, resize_hw)
            frames.append(frame_resized)

            target_ptr += 1
            if target_ptr >= len(indices):
                break
            target_idx = indices[target_ptr]

        cur_idx += 1

    cap.release()

    if len(frames) == 0:
        raise RuntimeError(f"Failed to sample frames from video: {video_path}")

    frames_np = np.stack(frames).astype("float32") / 255.0  # (T, H, W, C)
    frames_tensor = torch.from_numpy(frames_np)             # (T, H, W, C)
    frames_tensor = frames_tensor.permute(3, 0, 1, 2)       # (C, T, H, W)
    return frames_tensor


# =============================
# STEP 4: INFERENCE ON FRAMES
# =============================

def run_on_frames(frames_dir: Path,
                  yolo_model,
                  shot_model,
                  umpire_model,
                  runout_model):
    global SHOT_CLASSES, UMPIRE_CLASSES, RUNOUT_CLASSES

    # ðŸ”¹ Use the same subsampled frames as OCR
    frame_files = get_sampled_frame_paths(frames_dir, FRAME_SUBSAMPLE)
    print(f"Found {len(frame_files)} sampled frames for inference (subsample={FRAME_SUBSAMPLE}).")

    results = []

    for fpath in frame_files:
        frame = cv2.imread(str(fpath))
        if frame is None:
            print(f"WARNING: failed to read frame {fpath}")
            continue

        # Recover original frame index & time based on filename
        frame_index = frame_index_from_name(fpath)
        time_sec = frame_index * (1.0 / FRAME_RATE)

        # --- YOLO ---
        yolo_out = yolo_model(frame, verbose=False)
        detections = []
        if len(yolo_out) > 0:
            pred = yolo_out[0]
            for box in pred.boxes:
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                conf = float(box.conf[0])
                cls_id = int(box.cls[0])
                detections.append({
                    "bbox": [x1, y1, x2, y2],
                    "conf": conf,
                    "class_id": cls_id,
                    "class_name": pred.names.get(cls_id, str(cls_id)),
                })

        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        img_tensor = image_transform(img_rgb).unsqueeze(0).to(DEVICE)

        # --- Shot (kept for internal use, not used by LLM) ---
        with torch.no_grad():
            logits_shot = shot_model(img_tensor)
            probs_shot = torch.softmax(logits_shot, dim=1)[0]
            shot_prob, shot_idx = torch.max(probs_shot, dim=0)
            shot_label = SHOT_CLASSES[int(shot_idx)]
            shot_conf  = float(shot_prob)

        # --- Umpire (not used by LLM) ---
        with torch.no_grad():
            logits_ump = umpire_model(img_tensor)
            probs_ump = torch.softmax(logits_ump, dim=1)[0]
            ump_prob, ump_idx = torch.max(probs_ump, dim=0)
            ump_label = UMPIRE_CLASSES[int(ump_idx)]
            ump_conf  = float(ump_prob)

        # --- Runout (not used by LLM) ---
        with torch.no_grad():
            logits_run = runout_model(img_tensor)
            probs_run = torch.softmax(logits_run, dim=1)[0]
            run_prob, run_idx = torch.max(probs_run, dim=0)
            run_label = RUNOUT_CLASSES[int(run_idx)]
            run_conf  = float(run_prob)

        results.append({
            "frame_index": frame_index,
            "frame_path": str(fpath),
            "time_sec": time_sec,
            "yolo_detections": detections,
            "shot":   {"label": shot_label, "confidence": shot_conf},
            "umpire": {"label": ump_label, "confidence": ump_conf},
            "runout": {"label": run_label, "confidence": run_conf},
        })

    return results


# =============================
# STEP 5: INFERENCE ON CLIPS (R(2+1)D)
# =============================

def run_on_clips(clips_dir: Path, video_model):
    global VIDEO_CLASSES

    clip_files = sorted(clips_dir.glob("clip_*.mp4"))
    print(f"Found {len(clip_files)} clips for R(2+1)D.")

    results = []

    for cpath in clip_files:
        cname = cpath.name
        idx_str = cname.replace("clip_", "").replace(".mp4", "")
        try:
            clip_index = int(idx_str)
        except ValueError:
            clip_index = None

        start_time = clip_index * CLIP_LENGTH if clip_index is not None else None
        end_time   = start_time + CLIP_LENGTH if start_time is not None else None

        try:
            video_tensor = load_video_as_tensor(
                cpath, num_frames=16, resize_hw=(112, 112)
            )
        except Exception as e:
            print(f"ERROR reading clip {cpath}: {e}")
            continue

        video_tensor = video_tensor.unsqueeze(0).to(DEVICE)

        try:
            with torch.no_grad():
                logits = video_model(video_tensor)
                probs  = torch.softmax(logits, dim=1)[0]
                top_prob, top_idx = torch.max(probs, dim=0)
                label = VIDEO_CLASSES[int(top_idx)]
                conf  = float(top_prob)
        except Exception as e:
            print(f"ERROR running R(2+1)D on {cpath}: {e}")
            continue

        results.append({
            "clip_name": cname,
            "clip_path": str(cpath),
            "clip_index": clip_index,
            "start_time": start_time,
            "end_time": end_time,
            "video_class": {"label": label, "confidence": conf},
        })

    return results


# =============================
# STEP 6: BUILD TIMELINE (WITH OCR)
# =============================

def build_timeline(frame_results, clip_results, score_by_frame=None):
    """
    Build a time-ordered list of events.
    Each event carries SCOREBOARD OCR + YOLO + (optional) clip context.
    """
    clip_ranges = []
    for c in clip_results:
        st = c.get("start_time")
        et = c.get("end_time")
        if st is None or et is None:
            continue
        clip_ranges.append((st, et, c))
    clip_ranges.sort(key=lambda x: x[0])

    events = []
    for fr in frame_results:
        t = fr["time_sec"]
        clip_ctx = None
        for st, et, c in clip_ranges:
            if t >= st and t < et:
                clip_ctx = c
                break

        frame_path = fr["frame_path"]
        frame_name = Path(frame_path).name
        score_entry = score_by_frame.get(frame_name) if score_by_frame else None

        events.append({
            "time_sec": t,
            "frame_path": frame_path,
            "models": {
                "shot": fr["shot"],
                "umpire": fr["umpire"],
                "runout": fr["runout"],
                "yolo_detections": fr["yolo_detections"],
            },
            "score_ocr_text": (score_entry or {}).get("ocr_text"),
            "score_parsed": (score_entry or {}).get("parsed"),
            "clip_context": clip_ctx,
        })

    events.sort(key=lambda e: e["time_sec"])
    return events


# =============================
# STEP 8: BUILD LLM PROMPT + CALL JINA
# =============================

def _format_parsed_score(parsed):
    if not parsed:
        return "Unknown score"
    t1 = parsed.get("team1_name")
    s1 = parsed.get("team1_score", {}) or {}
    t2 = parsed.get("team2_name")
    s2 = parsed.get("team2_score", {}) or {}

    def fmt(team, score):
        if not team:
            return None
        runs = score.get("runs")
        w    = score.get("wickets")
        o    = score.get("overs")
        if runs is None and w is None and o is None:
            return team
        s = f"{team}: "
        if runs is not None:
            s += str(runs)
            if w is not None:
                s += f"/{w}"
        elif w is not None:
            s += f"-{w}"
        if o:
            s += f" in {o} overs"
        return s

    parts = []
    p1 = fmt(t1, s1)
    p2 = fmt(t2, s2)
    if p1:
        parts.append(p1)
    if p2:
        parts.append(p2)
    return " | ".join(parts) if parts else "Unknown score"


def build_commentary_prompt_from_timeline(timeline_events):
    """
    Build a prompt describing the entire innings as a time-ordered series of events.
    Continuous commentary â€“ no BALL 1 / BALL 2 labels.
    """
    lines = []

    lines.append(
        "You are given a time-ordered sequence of events from a cricket innings.\n"
        "Each event corresponds to a sampled frame and includes:\n"
        "- SCOREBOARD OCR (parsed runs/wickets/overs for the batting team).\n"
        "- YOLO detections describing the visual scene on the field.\n"
        "- An optional video-model label that gives a vague flavour of the shot type.\n\n"
        "HARD RULES:\n"
        "1. Treat the SCOREBOARD OCR as the ONLY source of truth for runs, wickets, and overs.\n"
        "2. Completely IGNORE all classifier outputs such as shot predictions, umpire gestures, and runout predictions. "
        "   Assume they are 'NOT AVAILABLE' and never use them to decide outcomes.\n"
        "3. Use YOLO detections ONLY to colour the visual description (batter, bowler, fielders, ball in air, boundary rope, etc.), "
        "   never to decide runs or wickets.\n"
        "4. When scorecard information is missing or unreliable at a time, do NOT invent exact scores. "
        "   Use safe, neutral commentary (for example, a dot ball or a generic defensive shot).\n"
        "5. When YOLO detections are empty, still write realistic but conservative commentary with no dramatic events.\n"
        "6. Do NOT mention OCR, detectors, models, probabilities, JSON, or any technical details.\n\n"
        "Your goal is to write continuous, live-style commentary over the innings, in chronological order, "
        "without labelling commentary as 'Ball 1', 'Ball 2', etc.\n"
    )

    for i, e in enumerate(timeline_events, start=1):
        t = e["time_sec"]
        score_parsed = e.get("score_parsed")
        score_str = _format_parsed_score(score_parsed) if score_parsed else "None"
        yolo_objs = e.get("models", {}).get("yolo_detections", []) or []
        clip_ctx = e.get("clip_context") or {}
        vc = clip_ctx.get("video_class") or {}
        vc_label = vc.get("label", "Unknown")
        vc_conf = vc.get("confidence", 0.0)

        lines.append(f"\nEVENT {i}:")
        lines.append(f" - Time (s): {t:.1f}")
        lines.append(f" - Scoreboard snapshot: {score_str}")
        lines.append(f" - Video model label (flavour only): {vc_label} (confidence={vc_conf:.2f})")
        lines.append(f" - Number of YOLO objects detected: {len(yolo_objs)}")

    lines.append(
        "\nTASK:\n"
        "Using the progression of the SCOREBOARD across these events, infer when runs are scored or wickets fall. "
        "Write a flowing, broadcast-style commentary that follows the innings from start to end. "
        "Keep the commentary chronological and natural, like a TV commentator describing the match live. "
        "Do not label individual balls or events; instead, produce a continuous narrative broken into sentences or short lines. "
        "Always stay consistent with the score changes and never invent impossible situations."
    )

    return "\n".join(lines)


def call_llm(prompt: str) -> str:
    api_key = os.getenv("JINA_API_KEY")
    if not api_key:
        print("[ERROR] JINA_API_KEY environment variable is not set.")
        return "[LLM ERROR] No Jina API key found. Skipping commentary generation."

    try:
        client = OpenAI(
            api_key=api_key,
            base_url=JINA_BASE_URL,
        )
    except Exception as e:
        print(f"[ERROR] Failed to create Jina OpenAI client: {e}")
        return "[LLM ERROR] Failed to create Jina client."

    system_msg = (
        "You are a professional live cricket commentator.\n"
        "You receive time-ordered structured data from a cricket innings. "
        "The only reliable source of runs, wickets, and overs is the SCOREBOARD OCR. "
        "YOLO detections and any video-model labels are used only for visual description.\n\n"
        "HARD CONSTRAINTS:\n"
        "- Completely IGNORE all shot-classification, umpire-gesture, and runout classifier outputs. "
        "  Act as if those predictions are not available.\n"
        "- Use SCOREBOARD OCR to determine how the score changes over time. "
        "  If the score is missing or unchanged for a period, do not invent exact numbers; "
        "  instead, use safe, neutral commentary.\n"
        "- Use YOLO detections only to enrich the visual scene (batterâ€™s stroke, fielders moving, ball running to boundary, etc.), "
        "  never to decide outcomes.\n"
        "- Never mention OCR, detectors, models, probabilities, JSON, or any technical terms.\n\n"
        "OUTPUT STYLE:\n"
        "- Produce a continuous stream of live commentary covering the whole innings in order.\n"
        "- Use realistic cricket language (strike rotation, pressure, partnerships, field placements, etc.).\n"
        "- Do NOT label commentary by ball numbers; just write natural sentences as if calling the match live."
        "based on the inputs from yolo generalize and give it need not to be accurate just a commentary if want synthesize it "
        "see if dont have anything to give continue from the previous chat provided by you need not to be accurate if no input from yolo and ocr is there"  
    )

    try:
        print(f"[INFO] Calling Jina model: {JINA_MODEL_ID}")
        chat = client.chat.completions.create(
            model=JINA_MODEL_ID,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt},
            ],
            temperature=0.9,
            max_tokens=800,
        )

        if chat.choices and chat.choices[0].message and chat.choices[0].message.content:
            return chat.choices[0].message.content

        return "[LLM WARNING] Jina call succeeded but no text was returned."
    except Exception as e:
        msg = str(e)
        if "524" in msg or "A timeout occurred" in msg or "timed out" in msg.lower():
            print("[ERROR] Jina DeepSearch request timed out (Cloudflare 524).")
            return "[LLM ERROR] Jina DeepSearch timed out while generating commentary. Try again or shorten the input."
        print(f"[ERROR] Jina LLM call failed: {e}")
        return "[LLM ERROR] Jina call failed. See console for details."


# ======================================================
# ðŸ”¹ SCORECARD OCR FUNCTIONS (OCR.SPACE PIPELINE)
# ======================================================

def crop_score_region(img: Image.Image):
    """Crop only the bottom score region based on fixed fraction coordinates."""
    width, height = img.size
    top = int(height * ROI_FRACTION_TOP)
    bottom = int(height * ROI_FRACTION_BOTTOM)
    return img.crop((0, top, width, bottom))


def image_to_base64_bytes(pil_img: Image.Image, quality=90):
    """Return base64-encoded JPEG bytes of a PIL image."""
    from io import BytesIO
    buf = BytesIO()
    pil_img.save(buf, format="JPEG", quality=quality)
    byte_data = buf.getvalue()
    return base64.b64encode(byte_data).decode("utf-8")


def call_vision_ocr(base64_image: str):
    """
    Calls the OCR.Space API using base64 image data.
    Returns dict: {"success": bool, "text": str, "error": str}
    """
    if not OCRSPACE_API_KEY or OCRSPACE_API_KEY == "YOUR_OCRSPACE_API_KEY":
        return {
            "success": False,
            "text": None,
            "error": "OCRSPACE_API_KEY not set. Please set env var or update config.",
        }

    base64_payload = f"data:image/jpeg;base64,{base64_image}"

    backoff = INITIAL_BACKOFF
    for attempt in range(1, MAX_RETRIES + 1):
        data = {
            "apikey": OCRSPACE_API_KEY,
            "language": "eng",
            "base64Image": base64_payload,
            "isOverlayRequired": False,
            "OCREngine": 2,
            "scale": True,
        }

        try:
            resp = requests.post(VISION_URL, data=data, timeout=60)
        except Exception as e:
            err = f"Network error: {e}"
            if attempt == MAX_RETRIES:
                return {"success": False, "text": None, "error": err}
            time.sleep(backoff)
            backoff *= 2
            continue

        if resp.status_code != 200:
            if resp.status_code in (429, 500, 503):
                retry_after = None
                if "Retry-After" in resp.headers:
                    try:
                        retry_after = float(resp.headers["Retry-After"])
                    except Exception:
                        pass
                wait = retry_after if retry_after is not None else backoff
                if attempt == MAX_RETRIES:
                    return {
                        "success": False,
                        "text": None,
                        "error": f"HTTP {resp.status_code}: {resp.text}",
                    }
                time.sleep(wait)
                backoff *= 2
                continue
            else:
                return {
                    "success": False,
                    "text": None,
                    "error": f"HTTP {resp.status_code}: {resp.text}",
                }

        try:
            data_json = resp.json()
        except Exception as e:
            return {
                "success": False,
                "text": None,
                "error": f"Invalid JSON response: {e}",
            }

        if data_json.get("IsErroredOnProcessing"):
            err_msg = data_json.get("ErrorMessage") or data_json.get("ErrorDetails")
            if isinstance(err_msg, list):
                err_msg = "; ".join(err_msg)
            return {"success": False, "text": None, "error": str(err_msg)}

        parsed_results = data_json.get("ParsedResults")
        if not parsed_results:
            return {
                "success": True,
                "text": "",
                "error": None,
            }

        text_chunks = []
        for pr in parsed_results:
            text_chunks.append(pr.get("ParsedText", "") or "")
        full_text = "\n".join(text_chunks).strip()

        return {"success": True, "text": full_text, "error": None}

    return {"success": False, "text": None, "error": "Max retries exceeded"}


# generic team pattern (fallback)
team_score_pattern = re.compile(
    r"(?P<team>[A-Za-z .&'()-]{2,40})\s+?(?P<runs>\d{1,4})\s*[\/\\-]?\s*(?P<wickets>\d{1,2})?\s*(?:\(|\[)?\s*(?P<overs>\d{1,2}(?:\.\d)?)?",
    re.IGNORECASE
)

# compact TV strip pattern (ENG 0-0 P 0.1/20 Toss ENG Smith 0* (4) Duckett 0 (0) WI 81 mph / 130 kph)
overlay_score_pattern = re.compile(
    r"\b(?P<bat_team>[A-Za-z]{2,4})\s+"
    r"(?P<runs>\d+)\s*[-/]\s*(?P<wickets>\d+)\s+"
    r"(?P<phase>[A-Za-z])?\s*"
    r"(?P<overs>\d+\.\d|\d+)\s*/\s*(?P<max_overs>\d+)\s+"
    r"Toss\s+(?P<toss_team>[A-Za-z]{2,4})\s+"
    r"(?P<striker>[A-Za-z]+)\s+(?P<striker_runs>\d+)\*?\s*\((?P<striker_balls>\d+)\)\s+"
    r"(?P<nonstriker>[A-Za-z]+)\s+(?P<nonstriker_runs>\d+)\s*\((?P<nonstriker_balls>\d+)\)\s+"
    r"(?P<bowl_team>[A-Za-z]{2,4})\s+"
    r"(?P<speed_mph>\d+)\s*mph\s*/\s*(?P<speed_kph>\d+)\s*kph",
    re.IGNORECASE
)


def parse_score_text(text):
    """
    1) Try compact TV overlay format.
    2) Fallback to generic 'TEAM 123/4 (12.3)' style.

    Returns backwards-compatible fields (team1/team2) plus extra overlay info.
    """
    txt = re.sub(r"\s+", " ", (text or "")).strip()

    out = {
        # existing structure used elsewhere
        "team1_name": None,
        "team2_name": None,
        "team1_score": {"runs": None, "wickets": None, "overs": None},
        "team2_score": {"runs": None, "wickets": None, "overs": None},

        # overlay-specific fields
        "overlay_type": None,
        "bat_team": None,
        "bowl_team": None,
        "phase": None,
        "overs_limit": None,
        "toss_team": None,
        "striker_name": None,
        "striker_runs": None,
        "striker_balls": None,
        "nonstriker_name": None,
        "nonstriker_runs": None,
        "nonstriker_balls": None,
        "speed_mph": None,
        "speed_kph": None,

        "raw_text": txt,
    }

    # ---- 1) compact overlay ----
    m_overlay = overlay_score_pattern.search(txt)
    if m_overlay:
        gd = m_overlay.groupdict()

        bat_team  = (gd.get("bat_team") or "").strip() or None
        bowl_team = (gd.get("bowl_team") or "").strip() or None
        toss_team = (gd.get("toss_team") or "").strip() or None

        runs      = int(gd["runs"]) if gd.get("runs") else None
        wickets   = int(gd["wickets"]) if gd.get("wickets") else None
        overs     = gd.get("overs") or None
        max_overs = int(gd["max_overs"]) if gd.get("max_overs") else None

        striker   = (gd.get("striker") or "").strip() or None
        s_runs    = int(gd["striker_runs"]) if gd.get("striker_runs") else None
        s_balls   = int(gd["striker_balls"]) if gd.get("striker_balls") else None

        nonstriker = (gd.get("nonstriker") or "").strip() or None
        ns_runs    = int(gd["nonstriker_runs"]) if gd.get("nonstriker_runs") else None
        ns_balls   = int(gd["nonstriker_balls"]) if gd.get("nonstriker_balls") else None

        speed_mph = int(gd["speed_mph"]) if gd.get("speed_mph") else None
        speed_kph = int(gd["speed_kph"]) if gd.get("speed_kph") else None

        out.update({
            "overlay_type": "compact_strip",
            "bat_team": bat_team,
            "bowl_team": bowl_team,
            "phase": gd.get("phase"),
            "overs_limit": max_overs,
            "toss_team": toss_team,
            "striker_name": striker,
            "striker_runs": s_runs,
            "striker_balls": s_balls,
            "nonstriker_name": nonstriker,
            "nonstriker_runs": ns_runs,
            "nonstriker_balls": ns_balls,
            "speed_mph": speed_mph,
            "speed_kph": speed_kph,
        })

        # Map batting team to team1 for compatibility
        out["team1_name"] = bat_team
        out["team2_name"] = bowl_team
        out["team1_score"]["runs"] = runs
        out["team1_score"]["wickets"] = wickets
        out["team1_score"]["overs"] = overs
        return out

    # ---- 2) generic team pattern ----
    matches = list(team_score_pattern.finditer(txt))

    if not matches:
        parts = re.split(r"\bvs\b|\bv\b|vs\.|\bv\.", txt, flags=re.IGNORECASE)
        if len(parts) >= 2:
            out["team1_name"] = parts[0].strip()
            out["team2_name"] = parts[1].strip().split()[0] if parts[1].strip() else None
        return out

    m1 = matches[0]
    out["team1_name"] = (m1.group("team") or "").strip()
    out["team1_score"]["runs"] = int(m1.group("runs")) if m1.group("runs") else None
    out["team1_score"]["wickets"] = int(m1.group("wickets")) if m1.group("wickets") else None
    out["team1_score"]["overs"] = m1.group("overs") if m1.group("overs") else None

    if len(matches) > 1:
        m2 = matches[1]
        out["team2_name"] = (m2.group("team") or "").strip()
        out["team2_score"]["runs"] = int(m2.group("runs")) if m2.group("runs") else None
        out["team2_score"]["wickets"] = int(m2.group("wickets")) if m2.group("wickets") else None
        out["team2_score"]["overs"] = m2.group("overs") if m2.group("overs") else None

    return out


def analyze_score_frame(image_path):
    """Crop frame bottom, send to OCR.Space, parse cricket score."""
    try:
        img = Image.open(image_path).convert("RGB")
        cropped = crop_score_region(img)

        base64_img = image_to_base64_bytes(cropped, quality=85)
        resp = call_vision_ocr(base64_img)

        if not resp["success"]:
            return {"error": resp["error"], "ocr_text": None, "parsed": None}

        text = resp["text"].strip() if resp["text"] else ""
        parsed = parse_score_text(text)

        return {"ocr_text": text, "parsed": parsed, "error": None}

    except Exception as e:
        return {"error": str(e), "ocr_text": None, "parsed": None}


def process_score_frames(frames_dir: Path,
                         output_json_path: Path,
                         output_csv_path: Path):
    """
    Run scorecard OCR on subsampled frames in frames_dir and save:
      - output_json_path : raw per-frame OCR/parsed data
      - output_csv_path  : flattened table (for easy viewing)
    Also returns the list of per-frame entries.
    """
    # ðŸ”¹ Use the same subsampled frames as models
    frame_paths = get_sampled_frame_paths(frames_dir, FRAME_SUBSAMPLE)
    print(f"Running scorecard OCR on {len(frame_paths)} frames in {frames_dir} (subsample={FRAME_SUBSAMPLE}) ...")

    results = []

    for f in tqdm(frame_paths, desc="Scorecard OCR"):
        f_str = str(f)
        r = analyze_score_frame(f_str)
        entry = {"frame": f.name}
        entry.update(r)
        results.append(entry)
        # tiny delay to avoid hammering free OCR.Space
        time.sleep(0.05)

    # Save JSON
    with open(output_json_path, "w", encoding="utf-8") as f_out:
        json.dump(results, f_out, indent=2, ensure_ascii=False)

    # Flatten to CSV
    rows = []
    for r in results:
        parsed = r.get("parsed") or {}
        rows.append({
            "frame": r.get("frame"),
            "ocr_text": (r.get("ocr_text") or "")[:500],
            "team1_name": parsed.get("team1_name"),
            "team1_runs": parsed.get("team1_score", {}).get("runs"),
            "team1_wickets": parsed.get("team1_score", {}).get("wickets"),
            "team1_overs": parsed.get("team1_score", {}).get("overs"),
            "team2_name": parsed.get("team2_name"),
            "team2_runs": parsed.get("team2_score", {}).get("runs"),
            "team2_wickets": parsed.get("team2_score", {}).get("wickets"),
            "team2_overs": parsed.get("team2_score", {}).get("overs"),
            "error": r.get("error"),
        })

    pd.DataFrame(rows).to_csv(output_csv_path, index=False, encoding="utf-8")

    print(f"\nâœ… Scorecard OCR results saved:")
    print(f"   JSON: {output_json_path}")
    print(f"   CSV : {output_csv_path}")

    return results


# =============================
# ElevenLabs TTS
# =============================

def synthesize_commentary_audio(commentary_text: str, output_path: Path):
    """
    Use ElevenLabs TTS to turn commentary text into an audio file.
    Requires ELEVENLABS_API_KEY and ELEVENLABS_VOICE_ID in env.
    """
    api_key = ELEVENLABS_API_KEY
    voice_id = ELEVENLABS_VOICE_ID

    if not api_key:
        print("[TTS] ELEVENLABS_API_KEY not set. Skipping TTS.")
        return False
    if not voice_id:
        print("[TTS] ELEVENLABS_VOICE_ID not set. Skipping TTS.")
        return False

    url = f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}"
    headers = {
        "xi-api-key": api_key,
        "Content-Type": "application/json",
        "Accept": "audio/mpeg",
    }
    payload = {
        "text": commentary_text,
        "model_id": ELEVENLABS_MODEL_ID,
    }

    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=120)
    except Exception as e:
        print(f"[TTS ERROR] Network error: {e}")
        return False

    if resp.status_code != 200:
        print(f"[TTS ERROR] HTTP {resp.status_code}: {resp.text[:300]}")
        return False

    try:
        with open(output_path, "wb") as f_out:
            f_out.write(resp.content)
        print(f"[TTS] Saved commentary audio to {output_path}")
        return True
    except Exception as e:
        print(f"[TTS ERROR] Failed to write audio file: {e}")
        return False


# =============================
# MAIN
# =============================

def main():
    global SHOT_CLASSES, UMPIRE_CLASSES, RUNOUT_CLASSES, VIDEO_CLASSES

    overall_start = time.time()
    stage_times = {}

    # --- STEP 1: Splitting video ---
    t0 = time.time()
    print("=== STEP 1: Splitting video with ffmpeg ===")
    run_ffmpeg_split(VIDEO_PATH, FRAMES_DIR, CLIPS_DIR, FRAME_RATE, CLIP_LENGTH)
    stage_times["ffmpeg_split"] = time.time() - t0

    # --- STEP 1b: Scorecard OCR ---
    t0 = time.time()
    print("=== STEP 1b: Running scorecard OCR on sampled frames ===")
    score_results = process_score_frames(FRAMES_DIR, SCORE_JSON, SCORE_CSV)
    stage_times["scorecard_ocr"] = time.time() - t0

    # Build quick lookup: frame_name -> score_entry
    score_by_frame = {
        entry["frame"]: entry
        for entry in score_results
        if entry.get("frame") is not None
    }

    # --- STEP 2: Load models ---
    t0 = time.time()
    print("=== STEP 2: Loading models ===")
    yolo_model = load_yolo_model(YOLO_WEIGHTS)

    shot_model,   SHOT_CLASSES   = load_efficientnet_classifier(SHOT_WEIGHTS,   SHOT_META_JSON)
    umpire_model, UMPIRE_CLASSES = load_umpire_model(UMPIRE_WEIGHTS,           UMPIRE_META_JSON)
    runout_model, RUNOUT_CLASSES = load_efficientnet_classifier(RUNOUT_WEIGHTS, RUNOUT_META_JSON)
    video_model,  VIDEO_CLASSES  = load_r2plus1d_model(R2P1D_WEIGHTS,          R2P1D_META_JSON)
    stage_times["load_models"] = time.time() - t0

    # --- STEP 3: Frame inference ---
    t0 = time.time()
    print("=== STEP 3: Inference on sampled frames ===")
    frame_results = run_on_frames(FRAMES_DIR, yolo_model, shot_model, umpire_model, runout_model)
    stage_times["frame_inference"] = time.time() - t0

    # --- STEP 4: Clip inference (R(2+1)D) ---
    t0 = time.time()
    print("=== STEP 4: Inference on clips (R(2+1)D) ===")
    clip_results = run_on_clips(CLIPS_DIR, video_model)
    stage_times["clip_inference"] = time.time() - t0

    # Save raw outputs
    t0 = time.time()
    all_outputs = {"frames": frame_results, "clips": clip_results}
    with open(RAW_RESULTS_JSON, "w") as f:
        json.dump(all_outputs, f, indent=2)
    print(f"Saved raw model outputs to {RAW_RESULTS_JSON}")
    stage_times["save_raw_outputs"] = time.time() - t0

    # --- STEP 5: Timeline (with OCR attached) ---
    t0 = time.time()
    print("=== STEP 5: Building timeline (match frames to clips + OCR) ===")
    timeline = build_timeline(frame_results, clip_results, score_by_frame=score_by_frame)
    with open(TIMELINE_JSON, "w") as f:
        json.dump({"events": timeline}, f, indent=2)
    print(f"Saved timeline to {TIMELINE_JSON}")
    stage_times["build_timeline"] = time.time() - t0

    # --- STEP 7: Build LLM prompt from timeline ---
    t0 = time.time()
    print("=== STEP 7: Building LLM prompt (timeline-based) ===")
    prompt = build_commentary_prompt_from_timeline(timeline)
    with open(PROMPT_TXT, "w", encoding="utf-8") as f:
        f.write(prompt)
    print(f"Saved LLM prompt to {PROMPT_TXT}")
    stage_times["build_prompt"] = time.time() - t0

    # --- STEP 8: LLM commentary ---
    t0 = time.time()
    print("=== STEP 8: Calling LLM for commentary ===")
    commentary = call_llm(prompt)
    stage_times["llm_commentary"] = time.time() - t0

    print("\n===== GENERATED COMMENTARY =====\n")
    print(commentary)

    # Optional: quick peek at last score entry
    if score_results:
        print("\n===== SAMPLE SCORECARD OCR ENTRY (last sampled frame) =====")
        print(json.dumps(score_results[-1], indent=2, ensure_ascii=False))

    # --- STEP 9: TTS ---
    t0 = time.time()
    if commentary and not commentary.startswith("[LLM ERROR]"):
        print("\n=== STEP 9: Converting commentary to audio (ElevenLabs) ===")
        synthesize_commentary_audio(commentary, TTS_OUTPUT)
    else:
        print("[TTS] Skipping TTS because commentary generation failed or returned an error.")
    stage_times["tts"] = time.time() - t0

    # --- Final latency report ---
    total_time = time.time() - overall_start
    print("\n===== PIPELINE LATENCY REPORT =====")
    for stage, secs in stage_times.items():
        print(f"{stage:20s}: {secs:6.2f} s")
    print(f"{'TOTAL (startâ†’end)':20s}: {total_time:6.2f} s")

    # Save to JSON for logging
    with open(BASE_DIR / "latency_report.json", "w") as f:
        json.dump({"stages": stage_times, "total_seconds": total_time}, f, indent=2)
        print(f"Saved latency report to {BASE_DIR / 'latency_report.json'}")


if __name__ == "__main__":
    main()
